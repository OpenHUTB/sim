\chapter{行人导航系统设计与实现}

\section{系统设计目标与需求}

本行人导航系统设计目标是在复杂城市环境中为行人提供有效导航支持，利用智能技术引导避开障碍物并高效到达目的地，为此系统须具备以下核心功能：根据用户提供的起点和终点坐标实时计算最优路径，综合考虑最短距离、行人行走速度、道路状况及交通规则等因素；具备实时障碍物检测与避让功能，及时识别并避开动态环境中的障碍物如其他行人、交通事故、施工区域等；持续监控周围环境并动态调整行进路线，应对突发障碍物或路线变化；具备极高实时响应能力，迅速感知环境变化并及时做出路径调整与决策。系统设计需在复杂动态环境中提供快速、精确、安全的行人导航服务。

\section{奖励函数设计}

强化学习任务中奖励函数设计至关重要，在行人导航系统中其直接影响智能体决策行为，合理设计奖励函数是实现高效稳定导航的关键，以下为本系统奖励函数的设计思想与实现细节：

\subsection{目标奖励}
目标奖励指行人接近目标位置时系统向智能体提供的奖励，旨在促使智能体朝目标前进，故奖励值随行人靠近目标而逐步增加，具体奖励计算方式如下：

\begin{equation}
R_{\text{target}} = - \frac{d_{\text{target}}}{100}
\end{equation}

其中，$d_{\text{target}}$表示行人当前位置到目标位置的距离。距离越近，奖励越大。

\subsection{避障奖励}
避障奖励指智能体在行进中成功避开障碍物时获得的奖励，通过激光雷达（Lidar）传感器检测周围障碍物距离并依据距离变化给出相应奖励，具体计算方式如下：

\begin{equation}
R_{\text{avoid}} = \max \left(0, 1 - \frac{d_{\text{obstacle}}}{5} \right)
\end{equation}

其中，$d_{\text{obstacle}}$表示行人当前与障碍物的最短距离，距离越大，奖励越小。

\subsection{动态调整奖励}
系统动态调整奖励旨在鼓励智能体依实时情况调整行进路线避突发障碍物，行人成功避障并调整路径时奖励增加，通过实时监控行人速度、路径调整和障碍物距离计算，计算公式如下：

\begin{equation}
R_{\text{dynamic}} = \alpha \cdot \left( \frac{v_{\text{current}}}{v_{\text{max}}} \right) \cdot \left( 1 - \frac{d_{\text{obstacle}}}{5} \right)
\end{equation}

其中，$v_{\text{current}}$表示当前速度，$v_{\text{max}}$表示最大速度，$d_{\text{obstacle}}$表示障碍物距离，$\alpha$为常数系数。

\subsection{时间惩罚与碰撞惩罚}
系统引入时间惩罚以控制导航时长并避免无效行走，每一步决策均对奖励值施加小幅度负惩罚；碰撞惩罚在智能体发生碰撞时施加较大负惩罚以确保安全性，具体公式如下：

\begin{equation}
R_{\text{time}} = - 0.01 \cdot t
\end{equation}

\begin{equation}
R_{\text{collision}} = - 30 \cdot \text{is\_collision}
\end{equation}

其中，$t$为时间步长，$\text{is\_collision}$为碰撞指示标志（若发生碰撞，则为1，否则为0）。

\subsection{综合奖励}
最终奖励为多个部分奖励加权和，综合考虑行人目标进展、避障效果、速度调整及碰撞惩罚等因素，最终奖励函数设计如下：

\begin{equation}
R_{\text{total}} = R_{\text{target}} + R_{\text{avoid}} + R_{\text{dynamic}} + R_{\text{time}} + R_{\text{collision}}
\end{equation}

通过该奖励函数的设计，行人导航系统能够有效地引导智能体在复杂环境中避开障碍物，迅速到达目标，并持续优化其决策策略，以实现最优路径规划。

\section{路径规划与避障设计}

路径规划与避障设计作为行人导航系统核心模块确保行人高效安全从起点抵达目标并避开动态环境障碍物，其实现依赖 Carla 仿真平台、强化学习（PPO 算法）及激光雷达（Lidar）和碰撞传感器等多种传感器输入。

\subsection{路径规划}

本系统路径规划部分通过强化学习计算起点到目标的最优路径，依赖行人当前状态、目标位置及环境障碍物信息，因需在动态环境中导航故需实时响应环境变化。强化学习训练时系统经反复试验学习最优路径选择，每一步依当前状态（含位置、朝向、速度、障碍物距离等）生成动作并据执行后的反馈学习，具体实施步骤为：根据行人当前状态计算与目标的方向和距离；使用 PPO 强化学习模型预测最佳动作（如直行、左转、右转等）；动作执行后系统实时监测环境确保行人不偏离目标并避开障碍物。路径规划中奖励函数设计（如目标奖励）助力智能体持续优化路径选择以抵达目标。

\subsection{避障设计}

避障设计通过实时监控周围障碍物距离确保行人前进中有效避障防碰撞，本系统实现避障依赖的具体传感器和算法如下：激光雷达（Lidar）传感器实时检测障碍物距离和位置以判断是否避让并计算最小距离，碰撞传感器在碰撞时触发、记录事件并更新状态，避障算法通过计算行人与障碍物距离实时调整速度和方向，障碍物较近时减速或暂停以避免碰撞。避障功能与路径规划相互配合，系统检测到障碍物时避障算法自动调整行人路线避开障碍区域，障碍物避开后路径规划模块重新计算最优路径确保行人继续朝目标前进。

在本系统中，避障功能与路径规划相互配合。当系统检测到障碍物时，避障算法会自动调整行人路线，避免进入障碍区域。一旦障碍物被避开，路径规划模块会重新计算最优路径，确保行人继续朝目标前进。

\subsection{路径规划与避障的协同工作}

路径规划与避障模块紧密配合以保障导航高效性和安全性，行人沿规划路径行进时系统通过激光雷达和碰撞传感器持续监控环境，检测到障碍物即执行避让操作，实时调整速度和方向避免碰撞，避障导致路径变化时路径规划模块即时重新计算最优路径并引导行人继续向目标行进，这种协同机制支持行人在复杂动态环境中稳定高效地完成导航任务。

\section{控制与优化}

本行人导航系统的控制与优化部分涵盖路径规划、避障策略控制及强化学习（PPO）模型优化。

\subsection{路径规划控制}
路径规划模块通过强化学习模型（PPO算法）来选择最优路径。系统根据行人当前的状态（位置、速度、方向）以及目标位置，计算最佳路径。通过优化策略，系统逐步减少行人到目标的距离，并避免与障碍物发生碰撞。

\subsection{避障控制}
避障控制通过实时感知环境（激光雷达与碰撞传感器），确保行人能够避开障碍物。传感器提供周围障碍物的信息，避障模块基于这些信息调整行人的速度和方向。在强化学习的帮助下，系统可以在复杂的动态环境中快速做出避让决策。

\subsection{强化学习优化}
PPO算法在系统中的优化过程包括通过大量的训练来不断调整行人的行为策略。每次行进，系统会根据实际反馈（奖励函数）更新策略，以便更高效、更安全地完成任务。PPO算法能够处理高维状态空间，优化行人决策，使得行人在不断变化的环境中能够适应复杂的导航需求。

\section{系统实现与测试}

\subsection{系统实现}

本行人导航系统基于 Carla 仿真平台并结合 PPO 强化学习算法实现路径规划与避障控制，主要模块如下：通过 Carla 客户端连接服务器并加载 “Town01” 地图，采用同步模式确保训练步骤中环境更新一致；利用 PPO 算法训练路径规划模型，依据当前状态预测最佳行进路径；借助激光雷达和碰撞传感器实时获取障碍物信息并调整路径以避免碰撞；通过 PPO 算法训练优化行人行为策略，提升行进效率与安全性。

\subsection{测试与验证}

测试通过多次模拟训练与演示验证系统在复杂环境中的导航能力，主要测试目标涵盖：验证系统能否依据不同起点和终点实时计算最优路径；测试系统能否有效避开障碍物并保障行人安全；考察系统长时间运行稳定性尤其是在多障碍物动态环境中的表现。
