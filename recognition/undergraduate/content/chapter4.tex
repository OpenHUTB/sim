%!TEX root = ../../csuthesis_main.tex
\chapter{基于通道注意力机制的CORnet-Z模型优化}

\section{引入注意力机制的动因}

\subsection{生物学基础：神经皮层的选择性响应}

在前述CORnet-S和CORnet-Z的模型结构中，各模块的特征提取过程采用标准卷积网络设计，未显式引入注意力调节机制。虽然该类模型在识别准确率与类脑性评估方面已具备一定表现，但在应对复杂图像背景、多目标干扰及特征稀疏性等问题时，模型关注能力有限，仍然存在注意力分配不足、无关区域激活过强的问题，导致信息利用效率降低，模型鲁棒性不足。

从神经科学的角度看，生物视觉系统能够通过注意力机制对有限的神经资源进行动态分配，这是感知过程中至关重要的一环。已有大量神经生理研究表明，大脑皮层不同层级的神经区域在视觉处理中表现出显著的选择性响应特征。例如，初级视觉皮层V1中的神经元更倾向于响应边缘、方向等低阶特征；而中层V4与高级区域IT则更侧重于颜色、形状和语义信息的整合处理。当个体的注意力被主动或被动地引导至某一目标区域时，与该区域相关的神经通道的活动将被显著增强，而与任务无关的信息通道则呈现出明显的抑制效应，这一机制称为“选择性增强/抑制”（selective enhancement/suppression）\cite{dicarlo2012does}。

为了模拟这一生物学机制，计算机视觉领域逐步发展出了多种注意力机制模块。Squeeze-and-Excitation（SE）模块的设计正是受这一神经调控模式启发。该模块通过“压缩”（squeeze）操作对空间维度进行全局信息整合，并通过“激励”（excitation）机制计算各通道的重要性分布，从而动态调整特征通道的响应强度，增强与目标任务相关的通道响应，抑制冗余干扰，具有高度类脑性的通道选择性功能\cite{hu2018squeeze}。

为此，本文在CORnet-Z模型基础上通过引入通道注意力机制（SE）模块，旨在提升模型对关键特征的感知能力和抗干扰能力，以更好地模拟神经系统的通道调控行为。这种结构调整有望在不显著增加模型复杂度的情况下，增强其在类脑评估任务中的表现一致性和识别准确率。

\subsection{工程动因：特征表示稀疏性与判别性}

从工程实践的角度看，传统的卷积神经网络模型在图像特征提取过程中普遍存在通道响应冗余与表征不具判别性的问题，尤其在网络结构相对浅层或轻量化模型中更为突出。这种问题主要表现为：网络在不同通道上学习到的特征可能存在较强相关性或信息重复，缺乏对关键类别特征的显著响应，从而导致计算资源浪费和模型性能瓶颈\cite{he2016deep}。具体而言，当模型对所有通道“一视同仁”地处理输入信息时，会在训练过程中平均分配注意力，导致有用特征通道未能获得足够强化，反而部分背景噪声或低语义信息通道被不必要地激活。这种响应冗余不仅增加了模型计算负担，还在推理阶段引入了错误响应风险，降低了分类准确率与泛化能力，特别在面对遮挡、复杂背景与小样本类别时表现尤为明显\cite{wang2020eca}。

针对上述问题，近年来研究者提出了多种注意力机制，以提升特征通道的筛选能力。通道注意力机制通过自动学习各通道的重要性分布，实现特征向量的“压缩与选择”，有效抑制无效激活、增强关键特征表达。其中，SE模块作为一种结构紧凑、参数极少的轻量级注意力机制，凭借其不引入额外空间维度、易于与主干网络并联嵌套的优势，在ResNet、MobileNet等主流网络中广泛应用\cite{hu2018squeeze}。SE模块通过两个阶段完成注意力权重的生成：首先在“Squeeze”阶段对每个通道进行全局平均池化，提取全局上下文信息；随后在“Excitation”阶段通过两层全连接网络学习非线性通道权重，并以Sigmoid函数输出注意力分布。最终，模型将原始特征图按通道注意力进行重新加权，实现“信息压缩+特征增强”的组合表达机制。这一设计思想与生物视觉系统中“有限资源优先调动”机制具有一定类脑对照性。

在多类目标混合、背景复杂的Tiny-ImageNet数据集上，模型对高层语义特征的表征能力直接决定了其分类效果。该数据集每类图像数量较少、类别间相似度高，模型必须有效区分小尺度目标与大背景之间的细粒度差异，传统卷积提取器在此场景中往往难以充分表达判别信息。本文在CORnet-Z模型的V4或IT层引入SE模块，目的就是在保持结构轻量性的同时，增强模型对关键语义区域的响应能力，提高特征稀疏性与判别强度。

实验结果表明，引入SE模块后模型在Top-1与Top-5准确率上均有所提升，说明通道选择性机制能够在高层抽象阶段有效强化目标核心区域特征表达，为后续类脑性一致性分析提供了优化基础。

\section{通道注意力模块设计与集成}

\subsection{SE结构原理与实现}

为了增强CORnet-Z模型在图像识别过程中的通道选择能力，本文在原始模型结构的基础上引入通道注意力SE模块，构建了改进模型CORnet-Z+SE。该机制通过显式建模通道之间的依赖关系，实现对冗余特征的压制与有效特征的增强，从而提升模型的判别能力与可解释性。

SE模块由Hu等人于2018年提出\cite{hu2018squeeze}，其核心思想就是通过两个阶段实现特征通道的重要性建模：squeeze（压缩）与 excitation（激励）。

在 squeeze 阶段，模块对输入特征图的每个通道进行全局平均池化，得到一个通道描述向量，压缩掉空间维度：

\begin{equation}
	z_c = 
	\frac{1}{H \times W} 
	\sum_{i=1}^{H} \sum_{j=1}^{W} x_c(i, j)
	\label{eq:se_squeeze}
\end{equation}

其中，$x_c(i,j)$表示输入特征图第$c$个通道在位置$(i,j)$的值，$z_c$为池化后通道$c$的全局描述，$H$和$W$分别为特征图的空间尺寸大小。

在excitation阶段，该通道向量$z$经过一个由两个全连接层组成的非线性变换，输出通道注意力权重向量$s$：

\begin{equation}
	s = \sigma \left( W_2 \cdot \text{ReLU} \left( W_1 \cdot z \right) \right)
	\label{eq:se_excitation}
\end{equation}

其中，$W_1 \in \mathbb{R}^{\frac{C}{r} \times C}$和$W_2 \in \mathbb{R}^{C \times \frac{C}{r}}$是两层全连接层的权重矩阵，$r$为压缩比超参数，用于控制中间层维度大小；$\text{ReLU}(\cdot)$表示线性整流函数，$\sigma(\cdot)$ 为Sigmoid 激活函数，用于将权重归一化到$[0,1]$区间。

最后，将注意力权重向量$s$与原始特征图逐通道相乘，完成对输入特征的加权重标定：

\begin{equation}
	\tilde{x}_c = s_c \cdot x_c
	\label{eq:se_scale}
\end{equation}

其中，$\tilde{x}_c$表示重标定后的通道特征图，$s_c$为通道$c$的注意力系数。通过这一机制，SE模块能够提升模型对目标区域特征的响应能力，增强判别特性。

在本文的实现中，使用PyTorch编写的\texttt{SEBlock}类对上述结构进行了复现。池化操作采用\texttt{nn.AdaptiveAvgPool2d(1)}，全连接层使用两层\texttt{Linear}模块分别对应压缩与激励过程，激活函数为ReLU和Sigmoid。参数初始化方面，前层使用\texttt{kaiming\_normal\_}，后层采用\texttt{normal\_}方法，以保证训练初期的稳定性。不同于原始论文中默认的压缩比$r=16$，本文选用更小的压缩比$r=8$，以减小信息损失并提高特征保持能力，使模块更适配于CORnet-Z这种浅层轻量模型结构。

\subsection{模块集成方式及参数配置}

在原始的CORnet-Z模型中，V1、V2、V4、IT四个模块以前馈方式逐层堆叠，每层结构为：Conv→ReLU→MaxPool。为在不重构网络的前提下集成注意力机制，本文在每一层中插入了SE模块，嵌入位置为：Conv→ReLU→SE→MaxPool。

即先进行特征提取与非线性激活，再由SE模块完成通道加权，最后下采样。此结构在\texttt{CORblock\_Z}中统一实现，并通过在初始化时设置\texttt{use\_se=True}启用。

各模块的通道设置如下表所示：

\begin{table}[htb]
	\centering
	\caption{CORnet-Z+SE各模块通道表}
	\label{tab:CORnet-Z+SE各模块通道表}
	\begin{tabular}{lllll}
		\hline
		模块& 输入通道 & 输出通道 \\
		\hline
		V1 & 3 & 64   \\
		V2 & 64 & 128   \\
		V4 & 128 & 256   \\
		IT & 256 & 512    \\
		\hline
	\end{tabular}
\end{table}

SE模块的参数均可被端到端训练自动学习，无需引入额外损失函数。集成后的模型结构保持可解释性良好，推理开销略高于原始的CORnet-Z模型，适用于中小型数据集（如Tiny-ImageNet-200）下的轻量化分类任务。

在模型训练中，保持原始CORnet-Z模型的训练策略不变，仅在结构上增加注意力机制，便于与原始模型在准确率与类脑性得分上进行对比分析。

\section{改进模型训练与性能表现}

\subsection{准确率与损失表现分析}

为评估所引入通道注意力机制对模型性能的实际影响，本文将改进后的CORnet-Z+SE模型与原始的CORnet-Z模型在Tiny-ImageNet-200数据集上的训练结果进行了系统对比与分析。重点分析模型在分类准确率、损失值以及收敛速度等方面的变化，用以验证SE模块的有效性。

在相同训练轮数范围内，原始的CORnet-Z模型在验证集上的Top-1准确率为32.51\%，Top-5准确率为59.21\%，对应的验证损失为2.9498。训练集表现略低，Top-1准确率仅为30.08\%，说明模型学习能力有限，可能存在梯度传播不充分或高层特征不具判别性的问题。

引入SE模块后的CORnet-Z+SE模型，在验证集上的Top-1准确率提升至36.66\%，Top-5准确率为62.72\%，损失下降至2.7265；训练集表现提升更为明显，Top-1准确率为35.94\%，Top-5准确率达64.84\%。验证与训练损失同步下降，说明注意力机制有效增强了模型的收敛能力和判别表达力。

\begin{table}[htb]
	\centering
	\caption{CORnet-Z与CORnet-Z+SE模型最佳性能表现对比}
	\label{tab:CORnet-Z与CORnet-Z+SE模型最佳性能表现对比}
	\begin{tabular}{lllll}
		\hline
		指标& CORnet-Z & CORnet-Z+SE \\
		\hline
		验证集Top-1准确率 & 32.51\% & 36.66\%  \\
		验证集Top-5准确率 & 59.21\% & 62.72\%  \\
		验证集损失值 & 2.9498 & 2.7265  \\
		训练集Top-1准确率 & 30.08\% & 35.94\%  \\
		训练集Top-5准确率 & 52.34\% & 64.84\%  \\
		训练集损失值 & 3.2815 & 2.7199  \\
		\hline
	\end{tabular}
\end{table}

从图像可视化分析来看，原始CORnet-Z模型如图\ref{f.zzxt}在训练过程中出现了较大的波动，尤其是Top-1准确率曲线抖动明显，表明模型的收敛过程不够平稳。而CORnet-Z+SE模型图\ref{f.zsezxt}在Top-1和Top-5准确率的上升曲线中的表现相较来说波动幅度更小，训练过程更稳定，验证集与训练集的性能差距也明显缩小，说明注意力机制的引入有助于特征通道的收敛与泛化。

同时，损失函数曲线（Loss Curve）也显示出一致趋势：原始的CORnet-Z模型在训练早期下降速度更快，但是最终损失值却略高，而且CORnet-Z原始模型在第5\textasciitilde10个epoch后出现训练集损失震荡，可能反映出部分通道未能有效建模关键区域。

\begin{figure}[hbt]
	\centering
	\includegraphics[width=0.9\linewidth]{cornet-z.png}
	\caption{CORnet-Z训练数据变化图}
	\label{f.zzxt}
\end{figure}

\begin{figure}[hbt]
	\centering
	\includegraphics[width=0.9\linewidth]{cornet-z-SE.png}
	\caption{CORnet-Z+SE训练数据变化图}
	\label{f.zsezxt}
\end{figure}

总体来看，SE模块带来的Top-1提升幅度为4.15\%，Top-5提升为3.51\%。虽然提升幅度有限，但考虑到模型结构变化极小，参数量变化可忽略，仍具有实际意义。尤其在训练集上的表现提升表明，注意力机制在训练早期能够加强对有效通道的识别，加快模型收敛速度。

\subsection{小结与分析}

通道注意力机制使模型能够根据全局信息动态调整各通道的响应强度，提升了模型对关键特征的表达能力。训练曲线显示，SE模型在准确率、损失值、学习曲线平稳性等方面均优于原始模型，在保持网络结构轻量的前提下实现了性能上的小幅提升。

不过，相较于大幅度结构重构或多尺度融合模型，本研究采用的改进方式较为保守，性能提升有限。这一结果也说明，通道注意力机制更适合作为辅助增强模块，而非单独主导模型性能的关键因素。
